{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>Ambient Temperature (in °C)</th>\n",
       "      <th>Delta Temp (Y)</th>\n",
       "      <th>Log (Y)</th>\n",
       "      <th>Temperature - 1 (in °C)</th>\n",
       "      <th>Delta Temp (Y-1)</th>\n",
       "      <th>North</th>\n",
       "      <th>East</th>\n",
       "      <th>West</th>\n",
       "      <th>South</th>\n",
       "      <th>Wind Impact (Should be linear variation &amp; wind speed)</th>\n",
       "      <th>FACE-A</th>\n",
       "      <th>FACE-B</th>\n",
       "      <th>FACE-C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>165</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>165</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>165</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>49</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>165</td>\n",
       "      <td>27</td>\n",
       "      <td>34</td>\n",
       "      <td>3.526361</td>\n",
       "      <td>58</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>165</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>62</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>41</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>15:30:00</td>\n",
       "      <td>180</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>35</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>15:30:00</td>\n",
       "      <td>180</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>32</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>15:30:00</td>\n",
       "      <td>180</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>44</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>32</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>15:30:00</td>\n",
       "      <td>180</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>30</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>15:30:00</td>\n",
       "      <td>180</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Temperature        Date      Time  wind_direction  \\\n",
       "0             44  2018-02-01  10:30:00             165   \n",
       "1             51  2018-02-01  10:30:00             165   \n",
       "2             53  2018-02-01  10:30:00             165   \n",
       "3             61  2018-02-01  10:30:00             165   \n",
       "4             65  2018-02-01  10:30:00             165   \n",
       "..           ...         ...       ...             ...   \n",
       "474           41  2018-02-10  15:30:00             180   \n",
       "475           35  2018-02-10  15:30:00             180   \n",
       "476           32  2018-02-10  15:30:00             180   \n",
       "477           32  2018-02-10  15:30:00             180   \n",
       "478           30  2018-02-10  15:30:00             180   \n",
       "\n",
       "     Ambient Temperature (in °C)  Delta Temp (Y)   Log (Y)  \\\n",
       "0                             27              17  2.833213   \n",
       "1                             27              24  3.178054   \n",
       "2                             27              26  3.258097   \n",
       "3                             27              34  3.526361   \n",
       "4                             27              38  3.637586   \n",
       "..                           ...             ...       ...   \n",
       "474                           27              14  2.639057   \n",
       "475                           27               8  2.079442   \n",
       "476                           27               5  1.609438   \n",
       "477                           27               5  1.609438   \n",
       "478                           27               3  1.098612   \n",
       "\n",
       "     Temperature - 1 (in °C)  Delta Temp (Y-1)  North  East  West  South  \\\n",
       "0                         40                13      0     0     1      0   \n",
       "1                         43                16      0     0     1      0   \n",
       "2                         49                22      0     0     0      1   \n",
       "3                         58                31      0     0     0      1   \n",
       "4                         62                35      0     0     0      1   \n",
       "..                       ...               ...    ...   ...   ...    ...   \n",
       "474                       37                10      0     0     0      1   \n",
       "475                       36                 9      0     0     0      1   \n",
       "476                       44                17      0     1     0      0   \n",
       "477                       38                11      1     0     0      0   \n",
       "478                       33                 6      1     0     0      0   \n",
       "\n",
       "     Wind Impact (Should be linear variation & wind speed)  FACE-A  FACE-B  \\\n",
       "0                                             0.000000           1       0   \n",
       "1                                             0.000000           0       1   \n",
       "2                                             0.965926           1       0   \n",
       "3                                             0.965926           0       1   \n",
       "4                                             0.965926           0       0   \n",
       "..                                                 ...         ...     ...   \n",
       "474                                           1.000000           1       0   \n",
       "475                                           1.000000           0       1   \n",
       "476                                           0.000000           1       0   \n",
       "477                                           0.000000           0       1   \n",
       "478                                           0.000000           0       0   \n",
       "\n",
       "     FACE-C  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         1  \n",
       "..      ...  \n",
       "474       0  \n",
       "475       0  \n",
       "476       0  \n",
       "477       0  \n",
       "478       1  \n",
       "\n",
       "[479 rows x 17 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('stockpile.csv', header=0)\n",
    "\n",
    "reqcols = list(data.columns)\n",
    "reqcols.remove('Temperature') # shuffing the order of the columns\n",
    "reqcols = ['Temperature'] + reqcols\n",
    "data = data[reqcols]\n",
    "pts, paras = data.shape\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial: ['10:30:00' '11:00:00' '11:30:00' '12:00:00' '12:30:00' '13:00:00'\n",
      " '10:00:00' '14:30:00' '15:00:00' '15:30:00' '13:30:00']\n",
      "transformed: [ 1  2  3  4  5  6  0  8  9 10  7]\n"
     ]
    }
   ],
   "source": [
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = LabelEncoder() \n",
    "unique_times =data['Time'].unique() \n",
    "print(\"initial:\", unique_times)\n",
    "data['Time']= label_encoder.fit_transform(data['Time']) \n",
    "print(\"transformed:\", label_encoder.transform(unique_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Temperature',\n",
       " 'Time',\n",
       " 'wind_direction',\n",
       " 'Ambient Temperature (in °C)',\n",
       " 'Delta Temp (Y)',\n",
       " 'Log (Y)',\n",
       " 'Temperature - 1 (in °C)',\n",
       " 'Delta Temp (Y-1)',\n",
       " 'North',\n",
       " 'East',\n",
       " 'West',\n",
       " 'South',\n",
       " 'Wind Impact (Should be linear variation & wind speed)',\n",
       " 'FACE-A',\n",
       " 'FACE-B',\n",
       " 'FACE-C']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqcols.remove('Date') # to be used in the next cell\n",
    "alldays = data.Date.unique()\n",
    "paras = paras-1 # 1 column is reduced\n",
    "reqcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.873603620294357, 0.3156306107959929, 1.9611953701419271, ...,\n",
       "        0.0, 0.0, '2018-02-01'],\n",
       "       [4.489858741704823, 0.3156306107959929, 1.9611953701419271, ...,\n",
       "        2.023638593649286, 0.0, '2018-02-01'],\n",
       "       [4.665931633536385, 0.3156306107959929, 1.9611953701419271, ...,\n",
       "        0.0, 0.0, '2018-02-01'],\n",
       "       ...,\n",
       "       [2.817166269304987, 3.156306107959929, 2.1394858583366476, ...,\n",
       "        0.0, 0.0, '2018-02-10'],\n",
       "       [2.817166269304987, 3.156306107959929, 2.1394858583366476, ...,\n",
       "        2.023638593649286, 0.0, '2018-02-10'],\n",
       "       [2.641093377473425, 3.156306107959929, 2.1394858583366476, ...,\n",
       "        0.0, 2.593778306244962, '2018-02-10']], dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdscl = StandardScaler(copy=False, with_mean = False)\n",
    "ct = ColumnTransformer([('somename',stdscl, reqcols)], remainder='passthrough')\n",
    "data = ct.fit_transform(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, epoch and iteration\n",
    "batch_sz = 16\n",
    "epochs = 160\n",
    "seq_ln = 8\n",
    "#intbatch = batch_sz/seq_ln # remember that seq_ln should divide batch_sz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "47\n",
      "47\n",
      "22\n",
      "22\n",
      "17\n",
      "17\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "strides = 2\n",
    "newtrm = np.array([]).reshape(0,seq_ln,  paras-1)\n",
    "newtym = np.array([]).reshape(0,seq_ln)\n",
    "\n",
    "for day in alldays:\n",
    "    index = data[:, 16] == day\n",
    "    ddt = data[index] \n",
    "    x = ddt[:, 1:-1]\n",
    "    y = ddt[:, 0]\n",
    "    pts = x.shape[0]\n",
    "    \n",
    "    newdatasz = (pts - seq_ln)//strides +1\n",
    "    newtr = np.zeros((newdatasz,seq_ln, paras-1))\n",
    "    newty = np.zeros((newdatasz,seq_ln))\n",
    "    print(newdatasz)\n",
    "    cc = 0\n",
    "    for i in range(newdatasz):\n",
    "        newtr[i] = x[cc: cc+seq_ln]\n",
    "        newty[i] = y[cc: cc+seq_ln]\n",
    "        cc += strides\n",
    "        \n",
    "    newtrm = np.vstack([newtrm, newtr])\n",
    "    newtym = np.vstack([newtym, newty])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strides are set to make sequences from 'strides' distance appart\n",
    "strides = 2\n",
    "x = data.values[:,0:-1]\n",
    "y = data.values[:, -1]\n",
    "\n",
    "newdatasz = (pts - seq_ln)//strides +1\n",
    "newtrm = np.zeros((newdatasz,seq_ln, paras-1))\n",
    "newtym = np.zeros((newdatasz,seq_ln))\n",
    "cc= 0\n",
    "for i in range(newdatasz):\n",
    "    newtrm[i] = x[cc: cc+seq_ln]\n",
    "    newtym[i] = y[cc: cc+seq_ln]\n",
    "    cc += strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(newtrm)\n",
    "y_train = torch.FloatTensor(newtym)\n",
    "y_trainlastst = y_train[:, -1] #change possible here\n",
    "\n",
    "train_data = TensorDataset(x_train,y_trainlastst)\n",
    "train_loader = DataLoader(train_data,batch_size=batch_sz,shuffle=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(RNNModel, self).__init__()  \n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, 1, batch_first=True, nonlinearity='tanh', dropout=0.2)\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.randn(1, x.size(0), self.hidden_dim))\n",
    "        # rnn n time steps\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        #print(out.shape)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(paras-1, 20)\n",
    "lossfn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.8, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training 16.50927734375\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_train)\n",
    "\n",
    "before_train = lossfn(y_pred.squeeze(), y_trainlastst)\n",
    "print('loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 :-- loss:= 6.323 :------------: r2_Accuracy:= 0.029\n",
      "Epoch: 10 :-- loss:= 0.149 :------------: r2_Accuracy:= 0.864\n",
      "Epoch: 20 :-- loss:= 0.086 :------------: r2_Accuracy:= 0.923\n",
      "Epoch: 30 :-- loss:= 0.062 :------------: r2_Accuracy:= 0.940\n",
      "Epoch: 40 :-- loss:= 0.056 :------------: r2_Accuracy:= 0.952\n",
      "Epoch: 50 :-- loss:= 0.047 :------------: r2_Accuracy:= 0.960\n",
      "Epoch: 60 :-- loss:= 0.036 :------------: r2_Accuracy:= 0.965\n",
      "Epoch: 70 :-- loss:= 0.033 :------------: r2_Accuracy:= 0.969\n",
      "Epoch: 80 :-- loss:= 0.028 :------------: r2_Accuracy:= 0.974\n",
      "Epoch: 90 :-- loss:= 0.025 :------------: r2_Accuracy:= 0.975\n",
      "Epoch: 100 :-- loss:= 0.020 :------------: r2_Accuracy:= 0.977\n",
      "Epoch: 110 :-- loss:= 0.026 :------------: r2_Accuracy:= 0.979\n",
      "Epoch: 120 :-- loss:= 0.019 :------------: r2_Accuracy:= 0.980\n",
      "Epoch: 130 :-- loss:= 0.018 :------------: r2_Accuracy:= 0.982\n",
      "Epoch: 140 :-- loss:= 0.020 :------------: r2_Accuracy:= 0.983\n",
      "Epoch: 150 :-- loss:= 0.016 :------------: r2_Accuracy:= 0.981\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for dx, dy in train_loader:     \n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(dx)\n",
    "        outputs = outputs.squeeze()\n",
    "        # Compute Loss\n",
    "        c_loss = lossfn(outputs, dy)\n",
    "        c_loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(c_loss.item())\n",
    "        loss += c_loss.item()\n",
    "        \n",
    "    if (epoch % 10 == 0):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x_train)\n",
    "            r2acc = r2_score(y_trainlastst, y_pred)\n",
    "            netloss = loss / len(train_loader)\n",
    "        print('Epoch: {} :-- loss:= {:.3f} :------------: r2_Accuracy:= {:.3f}'.format(epoch, netloss, r2acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>) nan\n",
      "1 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>) nan\n",
      "2 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>) nan\n",
      "3 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>) nan\n",
      "4 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>) nan\n",
      "5 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>) nan\n",
      "6 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>) nan\n",
      "7 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>) nan\n",
      "8 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>) nan\n",
      "9 tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>) nan\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs = model(x_train)\n",
    "    outputs = outputs.squeeze()\n",
    "    # Compute Loss\n",
    "    c_loss = lossfn(outputs, y_trainlastst)\n",
    "    c_loss.backward()\n",
    "    optimizer.step()\n",
    "    print(epoch, outputs, c_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5818906 4.401822\n",
      "3.3994255 3.3453848\n",
      "4.893186 4.8420043\n",
      "5.6711764 5.722369\n",
      "5.6762204 5.8104053\n",
      "4.535112 4.313786\n",
      "3.4011993 3.3453848\n",
      "4.6593127 4.4898586\n",
      "5.73778 5.8104053\n",
      "5.7320857 5.898442\n",
      "4.184369 3.9616401\n",
      "3.3620481 3.3453848\n",
      "5.454568 5.722369\n",
      "5.8569465 6.074515\n",
      "5.7681913 5.9864783\n",
      "4.16299 4.137713\n",
      "3.3407888 3.3453848\n",
      "5.551079 5.8104053\n",
      "5.9247994 6.2505875\n",
      "5.780849 6.074515\n",
      "3.8538775 3.7855673\n",
      "3.4704537 3.6094942\n",
      "5.3264213 5.546296\n",
      "5.716375 5.8104053\n",
      "5.4578414 5.546296\n",
      "3.4692998 3.521458\n",
      "3.1674094 3.2573485\n",
      "3.9591603 3.8736036\n",
      "3.1195102 3.0812757\n",
      "3.6412525 3.6094942\n",
      "5.0699406 4.930041\n",
      "4.6413984 4.577895\n",
      "3.9193192 3.7855673\n",
      "3.2183957 3.2573485\n",
      "3.8350472 3.7855673\n",
      "5.3866315 5.4582596\n",
      "5.06455 5.106114\n",
      "3.7980118 3.6975307\n",
      "3.3333879 3.4334214\n",
      "3.873982 3.8736036\n",
      "5.477498 5.546296\n",
      "5.2342625 5.282187\n",
      "3.5126195 3.4334214\n",
      "3.6097078 3.8736036\n",
      "4.2544513 4.2257495\n",
      "5.7399316 5.898442\n",
      "5.335271 5.370223\n",
      "3.9197693 3.9616401\n",
      "3.2074537 3.2573485\n",
      "4.7537136 4.8420043\n",
      "5.794005 5.9864783\n",
      "5.187816 5.1941504\n",
      "4.019902 4.137713\n",
      "3.391644 3.6094942\n",
      "4.1589513 4.313786\n",
      "5.6326184 5.898442\n",
      "4.9483185 5.106114\n",
      "3.4915986 3.6094942\n",
      "3.402947 3.6094942\n",
      "4.9303555 5.106114\n",
      "5.704723 5.8104053\n",
      "4.8995414 4.8420043\n",
      "2.70053 2.8171663\n",
      "3.0475678 3.0812757\n",
      "5.0465198 5.0180774\n",
      "5.5208945 5.370223\n",
      "4.7250633 4.577895\n",
      "2.9968476 2.9932392\n",
      "2.9156752 2.9052026\n",
      "4.6879754 4.4898586\n",
      "5.2423577 4.930041\n",
      "4.438295 4.2257495\n",
      "2.4616647 2.553057\n",
      "2.6641126 2.6410935\n",
      "4.317876 4.4898586\n",
      "3.720574 3.7855673\n",
      "3.7867785 3.6094942\n",
      "5.459664 5.370223\n",
      "4.9484906 4.6659317\n",
      "4.6267085 4.577895\n",
      "3.5627813 3.521458\n",
      "4.1622105 4.0496764\n",
      "5.6459284 5.722369\n",
      "5.180535 5.106114\n",
      "4.7671905 4.7539682\n",
      "3.5252075 3.521458\n",
      "4.5375934 4.4898586\n",
      "5.788394 5.9864783\n",
      "5.432527 5.4582596\n",
      "4.8670254 4.930041\n",
      "3.56979 3.6094942\n",
      "4.7302904 4.6659317\n",
      "5.776509 5.898442\n",
      "5.560162 5.6343327\n",
      "4.778622 4.7539682\n",
      "3.4951196 3.521458\n",
      "4.7503524 4.8420043\n",
      "5.8089533 6.074515\n",
      "5.3485494 5.370223\n",
      "4.6646156 4.8420043\n",
      "3.5836797 3.7855673\n",
      "4.7571197 4.930041\n",
      "5.691235 5.898442\n",
      "5.102534 5.1941504\n",
      "4.333519 4.577895\n",
      "3.3084474 3.4334214\n",
      "4.706343 4.8420043\n",
      "5.39469 5.370223\n",
      "4.6878123 4.6659317\n",
      "3.4620352 3.521458\n",
      "3.0982432 3.169312\n",
      "4.6923094 4.7539682\n",
      "5.3631835 5.370223\n",
      "4.1529403 4.0496764\n",
      "2.9681044 2.9932392\n",
      "3.1083355 3.169312\n",
      "4.583511 4.577895\n",
      "5.285173 5.282187\n",
      "3.9971051 3.9616401\n",
      "2.8764834 2.9052026\n",
      "2.8561492 2.8171663\n",
      "4.2377563 4.2257495\n",
      "3.501718 3.521458\n",
      "4.2164726 4.137713\n",
      "5.764334 5.9864783\n",
      "5.6312404 5.722369\n",
      "4.6599994 4.6659317\n",
      "3.6427078 3.6975307\n",
      "4.4591203 4.4898586\n",
      "5.7522616 5.898442\n",
      "5.143107 5.0180774\n",
      "4.3971763 4.4898586\n",
      "3.5129051 3.6094942\n",
      "4.7986455 4.930041\n",
      "5.620567 5.722369\n",
      "5.3174953 5.546296\n",
      "3.866959 3.9616401\n",
      "3.024959 3.0812757\n",
      "3.61348 3.4334214\n",
      "5.0437865 4.7539682\n",
      "5.3907657 5.546296\n",
      "3.084218 2.9932392\n",
      "2.9906096 2.9932392\n",
      "3.6664877 3.6094942\n",
      "3.1062498 2.9932392\n",
      "2.666141 2.6410935\n",
      "3.2412992 3.0812757\n",
      "2.9742823 2.9052026\n",
      "3.804562 3.8736036\n",
      "2.917122 2.8171663\n",
      "3.2182364 3.2573485\n",
      "3.5030923 3.4334214\n",
      "3.2752914 3.2573485\n",
      "3.4661412 3.4334214\n",
      "3.168025 3.169312\n",
      "2.6881123 2.7291298\n",
      "3.2475805 3.169312\n",
      "3.3089366 3.3453848\n",
      "3.1865606 3.169312\n",
      "2.677855 2.6410935\n",
      "2.5892816 2.6410935\n",
      "3.1205883 3.0812757\n",
      "2.8855128 2.8171663\n",
      "3.143083 3.169312\n",
      "2.6070285 2.553057\n",
      "3.592184 3.6094942\n",
      "4.2139177 4.2257495\n",
      "5.498585 5.722369\n",
      "5.018252 4.8420043\n",
      "3.8600674 3.6975307\n",
      "4.481142 4.401822\n",
      "5.6132426 5.8104053\n",
      "4.921409 4.7539682\n",
      "3.6441717 3.6094942\n",
      "4.556126 4.6659317\n",
      "5.2489552 5.282187\n",
      "4.5601273 4.577895\n",
      "3.171876 3.169312\n",
      "4.2516804 4.313786\n",
      "4.916679 4.7539682\n",
      "4.0700703 3.9616401\n",
      "2.940958 2.9052026\n",
      "3.7509828 3.6975307\n",
      "4.055069 4.0496764\n",
      "4.9902062 5.0180774\n",
      "5.1188407 5.282187\n",
      "3.9013672 3.7855673\n",
      "4.4721665 4.401822\n",
      "5.5737996 5.8104053\n",
      "5.220409 5.370223\n",
      "3.5781822 3.521458\n",
      "4.1466947 4.137713\n",
      "4.7861085 4.6659317\n",
      "4.7774415 4.930041\n",
      "3.2115092 3.169312\n",
      "3.305678 3.2573485\n",
      "3.9522762 3.7855673\n",
      "3.6049476 3.6975307\n",
      "2.9602342 2.9052026\n",
      "3.038515 2.8171663\n",
      "3.670514 3.521458\n",
      "4.2449703 4.137713\n",
      "3.6201181 3.521458\n",
      "3.1293454 2.9932392\n",
      "3.5752625 3.6094942\n",
      "4.1424537 4.0496764\n",
      "2.9291973 2.9052026\n",
      "3.1358647 3.0812757\n",
      "3.142353 3.169312\n",
      "3.468277 3.3453848\n",
      "2.868504 2.9052026\n",
      "2.9429994 2.9052026\n",
      "2.8847156 2.9052026\n",
      "3.1760683 3.0812757\n",
      "2.770728 2.8171663\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(x_train)\n",
    "    y_pred = y_pred.squeeze().numpy()\n",
    "y_tr = y_trainlastst.numpy()\n",
    "for i in range(len(y_pred)):\n",
    "    print(y_pred[i], y_tr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Absolute Error on Test:  0.096415535\n",
      "Average R^2 Value on Train:  0.9853970931285204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print ('Average Mean Absolute Error on Test: ', mean_absolute_error(y_tr, y_pred))\n",
    "print ('Average R^2 Value on Train: ', r2_score(y_tr, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, GRU, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom R2-score metrics for keras backend\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 12)                276       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 289\n",
      "Trainable params: 289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kmodel = Sequential()\n",
    "#Rnn layer\n",
    "kmodel.add(SimpleRNN(12, activation= 'relu', kernel_initializer='truncated_normal', input_shape=(seq_ln, paras-1)))\n",
    "#Output Layer\n",
    "kmodel.add(Dense(1, activation='relu', kernel_initializer='truncated_normal'))\n",
    "\n",
    "\n",
    "opt = Adam(lr=0.005)\n",
    "kmodel.compile(loss='mean_squared_error', optimizer=opt, metrics=[r2_keras])\n",
    "kmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 2s 19ms/step - loss: 1942.5447 - r2_keras: -28.7305\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 0s 447us/step - loss: 1920.9477 - r2_keras: -29.7625\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 0s 457us/step - loss: 1849.8037 - r2_keras: -29.2059\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 1676.6240 - r2_keras: -26.1398\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 0s 447us/step - loss: 1297.6757 - r2_keras: -18.9325\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 604.2467 - r2_keras: -9.0468\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 526.0825 - r2_keras: -6.8953\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 343.3511 - r2_keras: -4.6140\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 318.4481 - r2_keras: -4.5313\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 384.7758 - r2_keras: -5.6543\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 304.1912 - r2_keras: -3.6106\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 216.9417 - r2_keras: -2.3042\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 234.3455 - r2_keras: -2.4690\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 197.4389 - r2_keras: -2.0471\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 169.7258 - r2_keras: -1.5843\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 161.3976 - r2_keras: -1.5021\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 137.7193 - r2_keras: -1.2011\n",
      "Epoch 18/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 119.6317 - r2_keras: -0.8585\n",
      "Epoch 19/100\n",
      "107/107 [==============================] - 0s 401us/step - loss: 103.4295 - r2_keras: -0.5855\n",
      "Epoch 20/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 84.4301 - r2_keras: -0.4322\n",
      "Epoch 21/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 71.8809 - r2_keras: -0.0872\n",
      "Epoch 22/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 56.7499 - r2_keras: 0.1052\n",
      "Epoch 23/100\n",
      "107/107 [==============================] - 0s 447us/step - loss: 49.0675 - r2_keras: 0.2372\n",
      "Epoch 24/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 41.2565 - r2_keras: 0.3886\n",
      "Epoch 25/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 39.2844 - r2_keras: 0.4012\n",
      "Epoch 26/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 36.5382 - r2_keras: 0.4426\n",
      "Epoch 27/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 36.4806 - r2_keras: 0.4491\n",
      "Epoch 28/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 35.3585 - r2_keras: 0.4602\n",
      "Epoch 29/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 33.9298 - r2_keras: 0.4767\n",
      "Epoch 30/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 32.0788 - r2_keras: 0.5044\n",
      "Epoch 31/100\n",
      "107/107 [==============================] - 0s 503us/step - loss: 31.1670 - r2_keras: 0.5301\n",
      "Epoch 32/100\n",
      "107/107 [==============================] - 0s 522us/step - loss: 30.2119 - r2_keras: 0.5451\n",
      "Epoch 33/100\n",
      "107/107 [==============================] - 0s 475us/step - loss: 29.9229 - r2_keras: 0.5407\n",
      "Epoch 34/100\n",
      "107/107 [==============================] - 0s 457us/step - loss: 28.9262 - r2_keras: 0.5471\n",
      "Epoch 35/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 28.4370 - r2_keras: 0.5560\n",
      "Epoch 36/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 28.1916 - r2_keras: 0.5260\n",
      "Epoch 37/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 27.9064 - r2_keras: 0.5757\n",
      "Epoch 38/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 27.8244 - r2_keras: 0.5852\n",
      "Epoch 39/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 27.5044 - r2_keras: 0.5732\n",
      "Epoch 40/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 28.3928 - r2_keras: 0.5592\n",
      "Epoch 41/100\n",
      "107/107 [==============================] - 0s 447us/step - loss: 28.2359 - r2_keras: 0.5764\n",
      "Epoch 42/100\n",
      "107/107 [==============================] - 0s 474us/step - loss: 28.1113 - r2_keras: 0.5695\n",
      "Epoch 43/100\n",
      "107/107 [==============================] - 0s 457us/step - loss: 27.8666 - r2_keras: 0.5698\n",
      "Epoch 44/100\n",
      "107/107 [==============================] - 0s 446us/step - loss: 28.7710 - r2_keras: 0.5540\n",
      "Epoch 45/100\n",
      "107/107 [==============================] - 0s 447us/step - loss: 27.4109 - r2_keras: 0.5338\n",
      "Epoch 46/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 26.9308 - r2_keras: 0.5869\n",
      "Epoch 47/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 27.6560 - r2_keras: 0.5773\n",
      "Epoch 48/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 26.1634 - r2_keras: 0.6099\n",
      "Epoch 49/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 28.1395 - r2_keras: 0.5631\n",
      "Epoch 50/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 26.3812 - r2_keras: 0.5891\n",
      "Epoch 51/100\n",
      "107/107 [==============================] - 0s 475us/step - loss: 26.3572 - r2_keras: 0.5859\n",
      "Epoch 52/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 26.3582 - r2_keras: 0.5711\n",
      "Epoch 53/100\n",
      "107/107 [==============================] - 0s 447us/step - loss: 26.1748 - r2_keras: 0.5980\n",
      "Epoch 54/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 26.4997 - r2_keras: 0.5867\n",
      "Epoch 55/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 25.8519 - r2_keras: 0.5866\n",
      "Epoch 56/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 26.7724 - r2_keras: 0.5706\n",
      "Epoch 57/100\n",
      "107/107 [==============================] - 0s 447us/step - loss: 26.7967 - r2_keras: 0.5891\n",
      "Epoch 58/100\n",
      "107/107 [==============================] - 0s 466us/step - loss: 24.9361 - r2_keras: 0.6143\n",
      "Epoch 59/100\n",
      "107/107 [==============================] - 0s 494us/step - loss: 27.0687 - r2_keras: 0.5825\n",
      "Epoch 60/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 28.7786 - r2_keras: 0.5417\n",
      "Epoch 61/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 27.1339 - r2_keras: 0.5520\n",
      "Epoch 62/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 26.4284 - r2_keras: 0.6054\n",
      "Epoch 63/100\n",
      "107/107 [==============================] - 0s 457us/step - loss: 27.8832 - r2_keras: 0.5594\n",
      "Epoch 64/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 25.5369 - r2_keras: 0.5976\n",
      "Epoch 65/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 26.0542 - r2_keras: 0.5912\n",
      "Epoch 66/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 29.2380 - r2_keras: 0.5437\n",
      "Epoch 67/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 25.0213 - r2_keras: 0.6199\n",
      "Epoch 68/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 26.5489 - r2_keras: 0.6017\n",
      "Epoch 69/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 24.9807 - r2_keras: 0.6232\n",
      "Epoch 70/100\n",
      "107/107 [==============================] - 0s 466us/step - loss: 25.5087 - r2_keras: 0.6017\n",
      "Epoch 71/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 24.9909 - r2_keras: 0.6094\n",
      "Epoch 72/100\n",
      "107/107 [==============================] - 0s 457us/step - loss: 25.6911 - r2_keras: 0.6170\n",
      "Epoch 73/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 24.7290 - r2_keras: 0.6185\n",
      "Epoch 74/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 27.5387 - r2_keras: 0.5542\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 0s 419us/step - loss: 28.5203 - r2_keras: 0.5749\n",
      "Epoch 76/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 28.3863 - r2_keras: 0.5403\n",
      "Epoch 77/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 24.3347 - r2_keras: 0.6042\n",
      "Epoch 78/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 26.5997 - r2_keras: 0.5949\n",
      "Epoch 79/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 25.1059 - r2_keras: 0.6372\n",
      "Epoch 80/100\n",
      "107/107 [==============================] - 0s 447us/step - loss: 24.5890 - r2_keras: 0.6213\n",
      "Epoch 81/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 24.7249 - r2_keras: 0.5856\n",
      "Epoch 82/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 25.2606 - r2_keras: 0.5892\n",
      "Epoch 83/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 24.4427 - r2_keras: 0.5974\n",
      "Epoch 84/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 24.6397 - r2_keras: 0.6254\n",
      "Epoch 85/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 25.8247 - r2_keras: 0.5970\n",
      "Epoch 86/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 25.2269 - r2_keras: 0.5468\n",
      "Epoch 87/100\n",
      "107/107 [==============================] - 0s 485us/step - loss: 25.2019 - r2_keras: 0.5833\n",
      "Epoch 88/100\n",
      "107/107 [==============================] - 0s 475us/step - loss: 25.2376 - r2_keras: 0.6004\n",
      "Epoch 89/100\n",
      "107/107 [==============================] - 0s 419us/step - loss: 24.7158 - r2_keras: 0.6186\n",
      "Epoch 90/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 24.7840 - r2_keras: 0.5714\n",
      "Epoch 91/100\n",
      "107/107 [==============================] - 0s 429us/step - loss: 24.7084 - r2_keras: 0.6140\n",
      "Epoch 92/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 24.1976 - r2_keras: 0.6473\n",
      "Epoch 93/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 25.6463 - r2_keras: 0.6048\n",
      "Epoch 94/100\n",
      "107/107 [==============================] - 0s 457us/step - loss: 25.1037 - r2_keras: 0.6218\n",
      "Epoch 95/100\n",
      "107/107 [==============================] - 0s 447us/step - loss: 25.0894 - r2_keras: 0.5722\n",
      "Epoch 96/100\n",
      "107/107 [==============================] - 0s 438us/step - loss: 24.5793 - r2_keras: 0.6285\n",
      "Epoch 97/100\n",
      "107/107 [==============================] - 0s 457us/step - loss: 24.6053 - r2_keras: 0.6073\n",
      "Epoch 98/100\n",
      "107/107 [==============================] - 0s 447us/step - loss: 24.2180 - r2_keras: 0.6200\n",
      "Epoch 99/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 25.6488 - r2_keras: 0.5815\n",
      "Epoch 100/100\n",
      "107/107 [==============================] - 0s 410us/step - loss: 25.0584 - r2_keras: 0.6297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23036a61550>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmodel.fit(newtr,newty[:, -1],epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Absolute Error:  4.069523050183448\n",
      "Average R^2 Value on Train:  0.6400566485780137\n"
     ]
    }
   ],
   "source": [
    "y_pred = kmodel.predict(newtr)\n",
    "y_act = newty[:, -1]\n",
    "print ('Average Mean Absolute Error: ', mean_absolute_error(y_act, y_pred))\n",
    "print ('Average R^2 Value on Train: ', r2_score(y_act, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 12)                828       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 841\n",
      "Trainable params: 841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gmodel = Sequential()\n",
    "#Rnn layer\n",
    "gmodel.add(GRU(12, activation= 'relu', kernel_initializer='truncated_normal', input_shape=(seq_ln, paras-1)))\n",
    "#Output Layer\n",
    "gmodel.add(Dense(1, activation='relu', kernel_initializer='random_normal'))\n",
    "\n",
    "\n",
    "opt = Adam(lr=0.005)\n",
    "gmodel.compile(loss='mean_squared_error', optimizer=opt, metrics=[r2_keras])\n",
    "gmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 4s 34ms/step - loss: 1943.2464 - r2_keras: -29.9231\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 0s 941us/step - loss: 1935.4604 - r2_keras: -28.0592\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 0s 988us/step - loss: 1923.5488 - r2_keras: -31.0667\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 0s 960us/step - loss: 1902.3037 - r2_keras: -27.4627\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 1865.2657 - r2_keras: -28.0700\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 0s 969us/step - loss: 1797.4474 - r2_keras: -26.8168\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 1669.5833 - r2_keras: -24.4897\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 1424.1671 - r2_keras: -21.5731\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 0s 923us/step - loss: 980.3631 - r2_keras: -13.9816\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 0s 946us/step - loss: 413.2978 - r2_keras: -5.4572\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 0s 876us/step - loss: 441.1791 - r2_keras: -5.6963\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 0s 876us/step - loss: 314.1484 - r2_keras: -3.6967\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 0s 885us/step - loss: 257.4219 - r2_keras: -2.8846\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 0s 941us/step - loss: 269.8953 - r2_keras: -3.1566\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 0s 969us/step - loss: 222.5065 - r2_keras: -2.3904\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 0s 960us/step - loss: 188.7841 - r2_keras: -2.0968\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 198.2944 - r2_keras: -2.4396\n",
      "Epoch 18/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 171.7631 - r2_keras: -1.7460\n",
      "Epoch 19/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 161.5410 - r2_keras: -1.4985\n",
      "Epoch 20/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 135.0757 - r2_keras: -1.0400\n",
      "Epoch 21/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 122.6757 - r2_keras: -0.9447\n",
      "Epoch 22/100\n",
      "107/107 [==============================] - 0s 941us/step - loss: 100.7698 - r2_keras: -0.8264\n",
      "Epoch 23/100\n",
      "107/107 [==============================] - 0s 960us/step - loss: 98.0689 - r2_keras: -0.4701\n",
      "Epoch 24/100\n",
      "107/107 [==============================] - 0s 913us/step - loss: 79.0818 - r2_keras: -0.2217\n",
      "Epoch 25/100\n",
      "107/107 [==============================] - 0s 941us/step - loss: 76.7100 - r2_keras: -0.2112\n",
      "Epoch 26/100\n",
      "107/107 [==============================] - 0s 988us/step - loss: 65.9687 - r2_keras: -0.0210\n",
      "Epoch 27/100\n",
      "107/107 [==============================] - 0s 997us/step - loss: 51.8830 - r2_keras: 0.2244\n",
      "Epoch 28/100\n",
      "107/107 [==============================] - 0s 997us/step - loss: 49.4090 - r2_keras: 0.2237\n",
      "Epoch 29/100\n",
      "107/107 [==============================] - 0s 960us/step - loss: 45.7306 - r2_keras: 0.1988\n",
      "Epoch 30/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 43.6852 - r2_keras: 0.3104\n",
      "Epoch 31/100\n",
      "107/107 [==============================] - 0s 923us/step - loss: 42.9071 - r2_keras: 0.3437\n",
      "Epoch 32/100\n",
      "107/107 [==============================] - 0s 951us/step - loss: 46.8579 - r2_keras: 0.2674\n",
      "Epoch 33/100\n",
      "107/107 [==============================] - 0s 988us/step - loss: 42.1551 - r2_keras: 0.3536\n",
      "Epoch 34/100\n",
      "107/107 [==============================] - 0s 969us/step - loss: 37.1004 - r2_keras: 0.4145\n",
      "Epoch 35/100\n",
      "107/107 [==============================] - 0s 997us/step - loss: 36.2577 - r2_keras: 0.4565\n",
      "Epoch 36/100\n",
      "107/107 [==============================] - 0s 969us/step - loss: 36.0611 - r2_keras: 0.4203\n",
      "Epoch 37/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 31.2617 - r2_keras: 0.5305\n",
      "Epoch 38/100\n",
      "107/107 [==============================] - 0s 960us/step - loss: 30.7019 - r2_keras: 0.5480\n",
      "Epoch 39/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 31.5611 - r2_keras: 0.5155\n",
      "Epoch 40/100\n",
      "107/107 [==============================] - 0s 960us/step - loss: 27.9976 - r2_keras: 0.5332\n",
      "Epoch 41/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 28.5235 - r2_keras: 0.5347\n",
      "Epoch 42/100\n",
      "107/107 [==============================] - 0s 988us/step - loss: 27.4596 - r2_keras: 0.5886\n",
      "Epoch 43/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 25.4446 - r2_keras: 0.6162\n",
      "Epoch 44/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 24.7462 - r2_keras: 0.6115\n",
      "Epoch 45/100\n",
      "107/107 [==============================] - 0s 988us/step - loss: 25.4262 - r2_keras: 0.5873\n",
      "Epoch 46/100\n",
      "107/107 [==============================] - 0s 951us/step - loss: 24.2603 - r2_keras: 0.6194\n",
      "Epoch 47/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 23.4577 - r2_keras: 0.6424\n",
      "Epoch 48/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 22.4880 - r2_keras: 0.6338\n",
      "Epoch 49/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 23.7092 - r2_keras: 0.6154\n",
      "Epoch 50/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 22.4793 - r2_keras: 0.6461\n",
      "Epoch 51/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 21.3277 - r2_keras: 0.6291\n",
      "Epoch 52/100\n",
      "107/107 [==============================] - 0s 969us/step - loss: 21.4947 - r2_keras: 0.6647\n",
      "Epoch 53/100\n",
      "107/107 [==============================] - 0s 988us/step - loss: 21.7238 - r2_keras: 0.6609\n",
      "Epoch 54/100\n",
      "107/107 [==============================] - 0s 997us/step - loss: 22.5673 - r2_keras: 0.6496\n",
      "Epoch 55/100\n",
      "107/107 [==============================] - 0s 969us/step - loss: 21.0270 - r2_keras: 0.6552\n",
      "Epoch 56/100\n",
      "107/107 [==============================] - 0s 997us/step - loss: 20.9872 - r2_keras: 0.6766\n",
      "Epoch 57/100\n",
      "107/107 [==============================] - 0s 969us/step - loss: 20.9221 - r2_keras: 0.6811\n",
      "Epoch 58/100\n",
      "107/107 [==============================] - 0s 969us/step - loss: 21.7029 - r2_keras: 0.6753\n",
      "Epoch 59/100\n",
      "107/107 [==============================] - 0s 969us/step - loss: 21.0268 - r2_keras: 0.6843\n",
      "Epoch 60/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 20.6253 - r2_keras: 0.6715\n",
      "Epoch 61/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 20.8671 - r2_keras: 0.6803\n",
      "Epoch 62/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 20.3565 - r2_keras: 0.6848\n",
      "Epoch 63/100\n",
      "107/107 [==============================] - 0s 988us/step - loss: 19.4627 - r2_keras: 0.6796\n",
      "Epoch 64/100\n",
      "107/107 [==============================] - 0s 960us/step - loss: 18.6369 - r2_keras: 0.6952\n",
      "Epoch 65/100\n",
      "107/107 [==============================] - 0s 960us/step - loss: 19.6262 - r2_keras: 0.6722\n",
      "Epoch 66/100\n",
      "107/107 [==============================] - 0s 951us/step - loss: 18.6207 - r2_keras: 0.7152\n",
      "Epoch 67/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 19.2681 - r2_keras: 0.7063\n",
      "Epoch 68/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 19.3653 - r2_keras: 0.7103\n",
      "Epoch 69/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.8724 - r2_keras: 0.7147\n",
      "Epoch 70/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.7056 - r2_keras: 0.7223\n",
      "Epoch 71/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 18.9567 - r2_keras: 0.7023\n",
      "Epoch 72/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.1587 - r2_keras: 0.7359\n",
      "Epoch 73/100\n",
      "107/107 [==============================] - 0s 979us/step - loss: 17.5196 - r2_keras: 0.7312\n",
      "Epoch 74/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.7724 - r2_keras: 0.7224\n",
      "Epoch 75/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.1250 - r2_keras: 0.7235\n",
      "Epoch 76/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.1060 - r2_keras: 0.7415\n",
      "Epoch 77/100\n",
      "107/107 [==============================] - 0s 951us/step - loss: 19.5121 - r2_keras: 0.6966\n",
      "Epoch 78/100\n",
      "107/107 [==============================] - 0s 988us/step - loss: 16.5239 - r2_keras: 0.7384\n",
      "Epoch 79/100\n",
      "107/107 [==============================] - 0s 988us/step - loss: 18.7356 - r2_keras: 0.6446\n",
      "Epoch 80/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.1308 - r2_keras: 0.7654\n",
      "Epoch 81/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.4097 - r2_keras: 0.7556\n",
      "Epoch 82/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.4993 - r2_keras: 0.7411\n",
      "Epoch 83/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.1797 - r2_keras: 0.7348\n",
      "Epoch 84/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.0982 - r2_keras: 0.7453\n",
      "Epoch 85/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.8722 - r2_keras: 0.7318\n",
      "Epoch 86/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 15.7981 - r2_keras: 0.7567\n",
      "Epoch 87/100\n",
      "107/107 [==============================] - 0s 996us/step - loss: 15.9269 - r2_keras: 0.7341\n",
      "Epoch 88/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.1273 - r2_keras: 0.7378\n",
      "Epoch 89/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.2886 - r2_keras: 0.7364\n",
      "Epoch 90/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.7019 - r2_keras: 0.7014\n",
      "Epoch 91/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.5534 - r2_keras: 0.7251\n",
      "Epoch 92/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 15.4692 - r2_keras: 0.7622\n",
      "Epoch 93/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 15.1570 - r2_keras: 0.7685\n",
      "Epoch 94/100\n",
      "107/107 [==============================] - 0s 951us/step - loss: 15.2415 - r2_keras: 0.7737\n",
      "Epoch 95/100\n",
      "107/107 [==============================] - 0s 959us/step - loss: 18.9340 - r2_keras: 0.7214\n",
      "Epoch 96/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 18.7773 - r2_keras: 0.7123\n",
      "Epoch 97/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 18.6453 - r2_keras: 0.7165\n",
      "Epoch 98/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.7123 - r2_keras: 0.7174\n",
      "Epoch 99/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 15.9099 - r2_keras: 0.7347\n",
      "Epoch 100/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 15.1192 - r2_keras: 0.7455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23029b8d4a8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmodel.fit(newtr,newty[:, -1],epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Absolute Error:  2.980531675392222\n",
      "Average R^2 Value on Train:  0.7854226300513129\n"
     ]
    }
   ],
   "source": [
    "y_pred = gmodel.predict(newtr)\n",
    "y_act = newty[:, -1]\n",
    "print ('Average Mean Absolute Error: ', mean_absolute_error(y_act, y_pred))\n",
    "print ('Average R^2 Value on Train: ', r2_score(y_act, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 12)                1104      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,117\n",
      "Trainable params: 1,117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lmodel = Sequential()\n",
    "#Rnn layer\n",
    "lmodel.add(LSTM(12, activation= 'relu', kernel_initializer='truncated_normal', input_shape=(seq_ln, paras-1)))\n",
    "#Output Layer\n",
    "lmodel.add(Dense(1, activation='relu', kernel_initializer='random_normal'))\n",
    "\n",
    "\n",
    "opt = Adam(lr=0.005)\n",
    "lmodel.compile(loss='mean_squared_error', optimizer=opt, metrics=[r2_keras])\n",
    "lmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "107/107 [==============================] - 5s 43ms/step - loss: 1945.2051 - r2_keras: -28.2754\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 1933.8013 - r2_keras: -28.8019\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 1900.7029 - r2_keras: -28.2146\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 1757.3853 - r2_keras: -27.6346\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 1232.0690 - r2_keras: -19.8464\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 708.3362 - r2_keras: -10.0129\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 467.5947 - r2_keras: -6.1673\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 428.7596 - r2_keras: -5.4341\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 358.8090 - r2_keras: -4.7782\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 298.4931 - r2_keras: -3.5787\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 226.1233 - r2_keras: -2.5143\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 215.5368 - r2_keras: -2.3840\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 183.8100 - r2_keras: -1.9992\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 150.0053 - r2_keras: -1.4356\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 124.5331 - r2_keras: -0.8978\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 96.1545 - r2_keras: -0.4411\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 77.3531 - r2_keras: -0.2114\n",
      "Epoch 18/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 65.2852 - r2_keras: -0.0027\n",
      "Epoch 19/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 56.2468 - r2_keras: 0.1464\n",
      "Epoch 20/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 66.6863 - r2_keras: -0.0954\n",
      "Epoch 21/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 106.2128 - r2_keras: -0.7014\n",
      "Epoch 22/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 78.9789 - r2_keras: -0.4198\n",
      "Epoch 23/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 93.0203 - r2_keras: -0.4642\n",
      "Epoch 24/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 65.0723 - r2_keras: -0.0613\n",
      "Epoch 25/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 60.1469 - r2_keras: 0.0968\n",
      "Epoch 26/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 55.7689 - r2_keras: 0.1153\n",
      "Epoch 27/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 54.3876 - r2_keras: 0.1657\n",
      "Epoch 28/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 40.6883 - r2_keras: 0.3651\n",
      "Epoch 29/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 51.5022 - r2_keras: 0.2231\n",
      "Epoch 30/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 47.7440 - r2_keras: 0.2894\n",
      "Epoch 31/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 41.2761 - r2_keras: 0.3774\n",
      "Epoch 32/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 34.5756 - r2_keras: 0.4162\n",
      "Epoch 33/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 38.8639 - r2_keras: 0.3717A: 0s - loss: 40.1997 - r2_keras: 0.370\n",
      "Epoch 34/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 35.1199 - r2_keras: 0.4787\n",
      "Epoch 35/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 34.4879 - r2_keras: 0.4648\n",
      "Epoch 36/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 37.4913 - r2_keras: 0.4074\n",
      "Epoch 37/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 31.9136 - r2_keras: 0.5166\n",
      "Epoch 38/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 37.2888 - r2_keras: 0.4124\n",
      "Epoch 39/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 31.9928 - r2_keras: 0.5138\n",
      "Epoch 40/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 28.4105 - r2_keras: 0.4660\n",
      "Epoch 41/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 28.0659 - r2_keras: 0.5705\n",
      "Epoch 42/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 26.1734 - r2_keras: 0.6011\n",
      "Epoch 43/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 25.3530 - r2_keras: 0.6044\n",
      "Epoch 44/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 23.7481 - r2_keras: 0.6475\n",
      "Epoch 45/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 26.0402 - r2_keras: 0.5948\n",
      "Epoch 46/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 22.7509 - r2_keras: 0.6511\n",
      "Epoch 47/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 30.4800 - r2_keras: 0.5387\n",
      "Epoch 48/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 22.5381 - r2_keras: 0.6552\n",
      "Epoch 49/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 21.8842 - r2_keras: 0.6665\n",
      "Epoch 50/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 20.3718 - r2_keras: 0.6672\n",
      "Epoch 51/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 20.0530 - r2_keras: 0.6966\n",
      "Epoch 52/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 19.3609 - r2_keras: 0.6761\n",
      "Epoch 53/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 19.1244 - r2_keras: 0.7049\n",
      "Epoch 54/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 23.3683 - r2_keras: 0.5817\n",
      "Epoch 55/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 20.9799 - r2_keras: 0.6593\n",
      "Epoch 56/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 20.3663 - r2_keras: 0.6893\n",
      "Epoch 57/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 19.7719 - r2_keras: 0.6843\n",
      "Epoch 58/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 19.6442 - r2_keras: 0.6909\n",
      "Epoch 59/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 19.7680 - r2_keras: 0.6975\n",
      "Epoch 60/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 19.0675 - r2_keras: 0.6977\n",
      "Epoch 61/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 18.1449 - r2_keras: 0.6964\n",
      "Epoch 62/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 21.5598 - r2_keras: 0.6763\n",
      "Epoch 63/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 19.0515 - r2_keras: 0.7098\n",
      "Epoch 64/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 21.6612 - r2_keras: 0.6300\n",
      "Epoch 65/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 25.3685 - r2_keras: 0.5710\n",
      "Epoch 66/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.9452 - r2_keras: 0.7117\n",
      "Epoch 67/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 23.6675 - r2_keras: 0.6497\n",
      "Epoch 68/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 23.5379 - r2_keras: 0.6203\n",
      "Epoch 69/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 25.0162 - r2_keras: 0.6054\n",
      "Epoch 70/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 21.4887 - r2_keras: 0.6711\n",
      "Epoch 71/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 21.1284 - r2_keras: 0.6766\n",
      "Epoch 72/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 22.8247 - r2_keras: 0.6447\n",
      "Epoch 73/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 21.9046 - r2_keras: 0.6396\n",
      "Epoch 74/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.4249 - r2_keras: 0.7054\n",
      "Epoch 75/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 20.3107 - r2_keras: 0.6956\n",
      "Epoch 76/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 18.5954 - r2_keras: 0.6961\n",
      "Epoch 77/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.2926 - r2_keras: 0.7474\n",
      "Epoch 78/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 25.2074 - r2_keras: 0.6002\n",
      "Epoch 79/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 20.2446 - r2_keras: 0.6635\n",
      "Epoch 80/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 19.9145 - r2_keras: 0.6973\n",
      "Epoch 81/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 18.4828 - r2_keras: 0.7152\n",
      "Epoch 82/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 22.0448 - r2_keras: 0.6294\n",
      "Epoch 83/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.1634 - r2_keras: 0.7422\n",
      "Epoch 84/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.5356 - r2_keras: 0.7448\n",
      "Epoch 85/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 15.7218 - r2_keras: 0.7553\n",
      "Epoch 86/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.8363 - r2_keras: 0.7520\n",
      "Epoch 87/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.8825 - r2_keras: 0.7033\n",
      "Epoch 88/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.2924 - r2_keras: 0.7442\n",
      "Epoch 89/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 21.9920 - r2_keras: 0.6616\n",
      "Epoch 90/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 15.9919 - r2_keras: 0.7647\n",
      "Epoch 91/100\n",
      "107/107 [==============================] - ETA: 0s - loss: 15.7898 - r2_keras: 0.753 - 0s 1ms/step - loss: 15.4122 - r2_keras: 0.7617\n",
      "Epoch 92/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.0375 - r2_keras: 0.7549\n",
      "Epoch 93/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.8250 - r2_keras: 0.7295\n",
      "Epoch 94/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 17.6920 - r2_keras: 0.7219\n",
      "Epoch 95/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 21.3384 - r2_keras: 0.6623\n",
      "Epoch 96/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 19.4066 - r2_keras: 0.7085\n",
      "Epoch 97/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 16.3847 - r2_keras: 0.7445\n",
      "Epoch 98/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 15.2185 - r2_keras: 0.7783\n",
      "Epoch 99/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 18.7863 - r2_keras: 0.7157\n",
      "Epoch 100/100\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 18.5517 - r2_keras: 0.6955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2303accf240>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodel.fit(newtr,newty[:, -1],epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Absolute Error:  4.2723352385904185\n",
      "Average R^2 Value on Train:  0.5881351591619086\n"
     ]
    }
   ],
   "source": [
    "y_pred = lmodel.predict(newtr)\n",
    "y_act = newty[:, -1]\n",
    "print ('Average Mean Absolute Error: ', mean_absolute_error(y_act, y_pred))\n",
    "print ('Average R^2 Value on Train: ', r2_score(y_act, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
